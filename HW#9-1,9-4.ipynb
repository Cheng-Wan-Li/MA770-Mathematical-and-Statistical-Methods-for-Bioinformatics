{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MA770 Mathematical and Statistical Methods of Bioinformatics\n",
    "# Problem Set 9.1 & 9.4\n",
    "### Cheng, Wanli U31865818"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (a)  Basics: \n",
    "State the problem that the SVM is solving geometrically. What space are we in? What is the SVM optimizing? What role does the parameter $C$ play?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "SVM is aimed to solve the problems of classifying datapoints into 2 or more categories.\n",
    "\n",
    "Say the datapoints are represented by their *features* (i.e., the observables of which the values can be obtained from experiments). Then each data points can be denoted by a vector $\\mathbf{x}_i=(x^1_i,x^2_i,\\dots,x^d_i)$. In most cases, the experimental determined value of each feature can be parametrized in real numbers, then the vector $\\mathbf{x}_i$ actually lives in $\\mathbb{R}^d$, which is called the *feature space*. Therefore, we can geometrically visualize the dataset as a (finite/discrete) set of points in $\\mathbb{R}^d$. \n",
    "\n",
    "Under the circumstance of supervised learning, each point $\\mathbf{x}_i$ in the feature space is associate with a label $y_i$. And SVM is an algorithm trying to find the $d-1$ dimensional hyperplane(s) (hypersurfaces in general case) that separates the $\\mathbb{R}^d$ into regions s.t., (almost) all the data points belong to the same region has the same label.\n",
    "\n",
    "The performance of a particular model (i.e., a hypersurface) is evaluated in 2 aspects:\n",
    "\n",
    "$\\quad$ 1. The number of misclassified datapoints, i.e., the total error.\n",
    "\n",
    "$\\quad$ 2. The distance between different categories, i.e., the margin.\n",
    "\n",
    "Therefore the optimization of SVM is to first, minimizing the number of misclassified data points, and sencond, maximize the margin (or minimize the reciprocal of the margin).\n",
    "\n",
    "Since in different context, the importance of reduce misclassified data points and the importance to reach a good separation of the majority could be different, we define the *penalty parameter* $C$ as the weight of the total error in order to reach a balance between the two minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Margin: \n",
    "Given that you are in a 7,000 dimensional space, you might expect that you have a very sparse data set (e.g., imagine 4 data points in 3 dimensions). What would you expect this implies for the size of an optimal margin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data set is sparse, the distance between data points are relatively large. Therefore, if we use a small margin, there could be many possible way to place the hypersurface since the amount support vectors could be very small. In order to optimally separate the data (i.e., in a manner that could be properly generalized), we may use a relatively large margin, so we can use the slack variables to optimize the position and direction of our hypersurface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Slack variables:\n",
    "Define what slack variables are and their role in the SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By SVM, we are trying to find\n",
    "\\begin{equation}\n",
    "\\underset{f\\in\\mathcal{H}}{\\arg\\min}\\frac{1}{n}\\sum_{i=1}^{n}V(f(\\mathbf{x}_i),y_i)+\\lambda{\\parallel f\\parallel}^2_\\mathcal{H}\n",
    "\\end{equation}\n",
    "in general case.\n",
    "\n",
    "The slack varaibles are given by:\n",
    "$$\n",
    "\\xi_i\\equiv V(f(\\mathbf{x}_i),y_i)=(1-y_i f(\\mathbf{x}_i)).\n",
    "$$\n",
    "\n",
    "And we use it, $\\sum_i \\xi_i$, to express the total error of a particular choice of hypersurface and margin. Hence to train an SVM, we are trying to minimize $\\sum_i \\xi_i$ under an opotimal margin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Representation Theorem**\n",
    "\n",
    "A solution of the *Tikhonov optimization problem*\n",
    "\\begin{equation}\n",
    "\\underset{f\\in\\mathcal{H}}{\\arg\\min}\\frac{1}{n}\\sum_{i=1}^{n}V(f(\\mathbf{x}_i),y_i)+\\lambda{\\parallel f\\parallel}^2_\\mathcal{H}\n",
    "\\end{equation}\n",
    "can be written as\n",
    "\\begin{equation}\n",
    "f(\\mathbf{x})=\\sum_{i=1}^{n}a_i K(\\mathbf{x},\\mathbf{x}_i)\n",
    "\\end{equation}\n",
    "where $K$ denote the repreducing kernel of the *RHKS* $\\mathcal{H}$ (in particular, $\\mathcal{H}$ is the *Hilbert space* of real-valued functions on a set $X$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proof:**\n",
    "\n",
    "Use calculus of variations. If there exists a function $f_0$ that minimize the equation:\n",
    "\\begin{equation}\n",
    "\\underset{f\\in\\mathcal{H}}{\\arg\\min}\\frac{1}{n}\\sum_{i=1}^{n}V(f(\\mathbf{x}_i),y_i)+\\lambda{\\parallel f\\parallel}^2_\\mathcal{H},\n",
    "\\end{equation}\n",
    "then for all function $g\\in\\mathcal{H}$, we have (assuming that the derivatives with respect to $\\epsilon$ exist) :\n",
    "\\begin{align}\n",
    "0&=\\frac{d}{d\\epsilon}[\\frac{1}{n}\\sum_{i=1}^{n}V((f_0+\\epsilon g)(\\mathbf{x}_i),y_i)+\\lambda{\\parallel f_0+\\epsilon g\\parallel}^2_\\mathcal{H}]_{f=f_0} \\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{dV}{d\\epsilon}+\\left.\\lambda\\frac{d{\\parallel f_0+\\epsilon g \\parallel}^2}{d\\epsilon}\\right|_{\\epsilon=0}\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial V}{\\partial f(\\mathbf{x}_i)}\\frac{d f(\\mathbf{x}_i)}{d\\epsilon}+\\left.\\lambda\\frac{d{\\langle f_0+\\epsilon g,f_0+\\epsilon g\\rangle}}{d\\epsilon}\\right|_{\\epsilon=0}\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial V}{\\partial f(\\mathbf{x}_i)}\\left.\\frac{d f_0(\\mathbf{x}_i)+\\epsilon g(\\mathbf{x}_i)}{d\\epsilon}\\right|_{\\epsilon=0}+\\left.\\lambda\\frac{d{\\langle f_0 \\rangle}^2+2\\epsilon\\langle f_0,g\\rangle+\\epsilon^2{\\langle g\\rangle}^2}{d\\epsilon}\\right|_{\\epsilon=0}\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial V}{\\partial f(\\mathbf{x}_i)}\\cdot g(\\mathbf{x}_i)+\\lambda[2\\langle f_0,g\\rangle+2\\epsilon{\\langle g\\rangle}^2]_{\\epsilon=0}\\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial V}{\\partial f(\\mathbf{x}_i)}\\cdot g(\\mathbf{x}_i)+2\\lambda\\langle f_0,g\\rangle.\n",
    "\\end{align}\n",
    "\n",
    "Let $V_1$ denote the partial derivative of $V$ for the first varaible, i.e., $\\frac{\\partial V}{\\partial f(\\mathbf{x}_i)}$, we can write the equation as:\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}V_1(f_0(\\mathbf{x}_i),y_i)\\cdot g(\\mathbf{x}_i)+2\\lambda\\langle f_0,g\\rangle=0.\n",
    "$$\n",
    "\n",
    "The above equation works for any $g\\in\\mathcal{H}$, \n",
    "\n",
    "Let the *reproducing kernel* of  $\\mathcal{H}$ denote by:\n",
    "$$\n",
    "K(\\mathbf{x},\\mathbf{y})\\equiv K_\\mathbf{x}(\\mathbf{y})\n",
    "$$\n",
    "where $K_\\mathbf{x}$ denote the function derived from $K(\\mathbf{x},\\mathbf{y})$ with $\\mathbf{x}$ pre-determined. And the existence for $K_\\mathbf{x}$ with respect to any $\\mathbf{x}\\in X$ is guaranteed by Riesz representation theorem.\n",
    "\n",
    "Since the equation we derived above works for any function $g$ in $\\mathcal{H}$, it certainly works for $K_\\mathbf{x}$, i.e.,\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}V_1(f_0(\\mathbf{x}_i),y_i)\\cdot K_\\mathbf{x}(\\mathbf{x}_i)+2\\lambda\\langle f_0,K_\\mathbf{x}\\rangle=0.\n",
    "$$\n",
    "\n",
    "By definition of an RKHS, we have:\n",
    "$$\n",
    "\\langle f_0,K_\\mathbf{x}\\rangle=f_0(\\mathbf{x}),\n",
    "$$\n",
    "therefore, the we can write:\n",
    "\\begin{align}\n",
    "2\\lambda\\langle f_0,K_\\mathbf{x}\\rangle&=-\\frac{1}{n}\\sum_{i=1}^{n}V_1(f_0(\\mathbf{x}_i),y_i)\\cdot K_\\mathbf{x}(\\mathbf{x}_i)\\\\\n",
    "f_0(\\mathbf{x})&=-\\frac{1}{2\\lambda n}\\sum_{i=1}^{n}V_1(f_0(\\mathbf{x}_i),y_i)\\cdot K(\\mathbf{x},\\mathbf{x}_i).\n",
    "\\end{align}\n",
    "\n",
    "Then, by packing up the expression in the following convention:\n",
    "$$\n",
    "a_i=-\\frac{1}{2\\lambda n}V_1(f_0(\\mathbf{x}_i),y_i),\n",
    "$$\n",
    "we have:\n",
    "$$\n",
    "f_0(\\mathbf{x})=\\sum_{i=1}^{n}a_i K(\\mathbf{x},\\mathbf{x}_i),\n",
    "$$\n",
    "which is in the form we want.\n",
    "\n",
    "Since $a_i$ explicitly depends on choice of $f_0$, the result we get above does not really solve the problem. However, the reasoning here heuristically shows why we expect $f_0$ to take the form\n",
    "$\\sum_{i=1}^{n}a_i K(\\mathbf{x},\\mathbf{x}_i)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
